{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
        "\n",
        "#### **Simple Linear Regression**:\n",
        "- Simple Linear Regression models the relationship between a **single** independent variable (**X**) and a **dependent** variable (**Y**).\n",
        "- It assumes a linear relationship between the two variables and fits a straight line to the data.\n",
        "- The equation for simple linear regression is:\n",
        "  \n",
        "  \\[\n",
        "  Y = b_0 + b_1X + \\epsilon\n",
        "  \\]\n",
        "\n",
        "  where:\n",
        "  - \\( Y \\) = Dependent variable (target)\n",
        "  - \\( X \\) = Independent variable (feature)\n",
        "  - \\( b_0 \\) = Intercept\n",
        "  - \\( b_1 \\) = Slope (coefficient for \\( X \\))\n",
        "  - \\( \\epsilon \\) = Error term\n",
        "\n",
        "#### **Example of Simple Linear Regression**:\n",
        "- Predicting house price based on square footage:\n",
        "\n",
        "  \\[\n",
        "  \\text{Price} = b_0 + b_1 (\\text{Square Footage}) + \\epsilon\n",
        "  \\]\n",
        "\n",
        "---\n",
        "\n",
        "#### **Multiple Linear Regression**:\n",
        "- Multiple Linear Regression models the relationship between **two or more** independent variables (**X₁, X₂, X₃, ...**) and a **dependent** variable (**Y**).\n",
        "- It extends simple linear regression by incorporating multiple predictors to improve accuracy.\n",
        "- The equation for multiple linear regression is:\n",
        "\n",
        "  \\[\n",
        "  Y = b_0 + b_1X_1 + b_2X_2 + b_3X_3 + ... + b_nX_n + \\epsilon\n",
        "  \\]\n",
        "\n",
        "  where:\n",
        "  - \\( X_1, X_2, ..., X_n \\) are multiple independent variables (features).\n",
        "  - \\( b_1, b_2, ..., b_n \\) are the corresponding coefficients.\n",
        "\n",
        "#### **Example of Multiple Linear Regression**:\n",
        "- Predicting house price based on multiple factors:\n",
        "\n",
        "  \\[\n",
        "  \\text{Price} = b_0 + b_1 (\\text{Square Footage}) + b_2 (\\text{Number of Bedrooms}) + b_3 (\\text{Location Score}) + \\epsilon\n",
        "  \\]\n",
        "\n",
        "#### **Conclusion**:\n",
        "- We Use **Simple Linear Regression** when there is only **one predictor**.\n",
        "- We Use **Multiple Linear Regression** when multiple variables influence the dependent variable.\n"
      ],
      "metadata": {
        "id": "2ryR8rJsbIBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
        "\n",
        "Linear regression relies on several key assumptions to ensure accurate predictions. These assumptions must be validated before interpreting results.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Linearity**  \n",
        "The relationship between independent and dependent variables must be linear.  \n",
        "**Check:** Scatter plots, residual plots.  \n",
        "**Fix:** Polynomial regression, log transformation.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Independence of Errors (No Autocorrelation)**  \n",
        "Residuals should not be correlated.  \n",
        "**Check:** Durbin-Watson test, residual plots.  \n",
        "**Fix:** Use time series models, add lag variables.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Homoscedasticity (Constant Variance of Errors)**  \n",
        "Residuals should have constant variance.  \n",
        "**Check:** Residual vs. predicted plots, Breusch-Pagan test.  \n",
        "**Fix:** Log transformation, weighted least squares regression.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Normality of Residuals**  \n",
        "Residuals should be normally distributed.  \n",
        "**Check:** Histogram, Q-Q plot, Shapiro-Wilk test.  \n",
        "**Fix:** Log/square root transformation, Box-Cox transformation.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. No Multicollinearity (For Multiple Regression)**  \n",
        "Independent variables should not be highly correlated.  \n",
        "**Check:** Correlation matrix, VIF (VIF > 5 suggests multicollinearity).  \n",
        "**Fix:** Remove correlated variables, use PCA or Ridge regression.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**  \n",
        "Verifying these assumptions ensures a reliable model. If violated, apply transformations or alternative models like polynomial or Ridge regression.\n"
      ],
      "metadata": {
        "id": "u_sUR3QsblDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
        "\n",
        "In a linear regression model, the equation is typically represented as:\n",
        "\n",
        "\\[\n",
        "Y = \\beta_0 + \\beta_1 X + \\epsilon\n",
        "\\]\n",
        "\n",
        "where:  \n",
        "- \\( Y \\) is the dependent variable (target).  \n",
        "- \\( X \\) is the independent variable (predictor).  \n",
        "- \\( \\beta_0 \\) (Intercept) is the predicted value of \\( Y \\) when \\( X = 0 \\).  \n",
        "- \\( \\beta_1 \\) (Slope) represents the change in \\( Y \\) for a one-unit increase in \\( X \\).  \n",
        "- \\( \\epsilon \\) is the error term.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example: Predicting House Prices**  \n",
        "Suppose we build a regression model to predict house prices (₹Y) based on square footage (X). The model equation is:\n",
        "\n",
        "\\[\n",
        "\\text{Price} = 10,00,000 + 2,000 \\times \\text{Square Footage}\n",
        "\\]\n",
        "\n",
        "- **Intercept (₹10,00,000)**: This means that if a house had 0 square feet (hypothetically), its predicted price would be ₹10,00,000. While not always meaningful, it serves as the base value.  \n",
        "- **Slope (₹2,000)**: For every additional square foot of house size, the price increases by ₹2,000.\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**  \n",
        "The slope shows the rate of change, while the intercept gives the baseline prediction. Interpreting them correctly helps in making informed decisions.\n"
      ],
      "metadata": {
        "id": "d3e-SvpLdZQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
        "\n",
        "**Gradient Descent** is an optimization algorithm used to minimize a function by iteratively adjusting its parameters in the direction of the steepest descent (negative gradient). It is widely used in machine learning to optimize models by reducing the error (loss function).\n",
        "\n",
        "---\n",
        "\n",
        "### **How It Works**\n",
        "1. **Initialize Parameters**: Start with random values for model parameters (e.g., weights in a regression or neural network).\n",
        "2. **Compute Gradient**: Calculate the gradient (derivative) of the loss function with respect to each parameter.\n",
        "3. **Update Parameters**: Adjust the parameters using the formula:\n",
        "\n",
        "   \\[\n",
        "   \\theta = \\theta - \\alpha \\frac{\\partial J}{\\partial \\theta}\n",
        "   \\]\n",
        "\n",
        "   where:  \n",
        "   - \\( \\theta \\) represents model parameters.  \n",
        "   - \\( J \\) is the loss function.  \n",
        "   - \\( \\alpha \\) is the learning rate (step size).  \n",
        "   - \\( \\frac{\\partial J}{\\partial \\theta} \\) is the gradient.\n",
        "\n",
        "4. **Repeat Until Convergence**: The process continues until the loss function reaches a minimum.\n",
        "\n",
        "---\n",
        "\n",
        "### **Types of Gradient Descent**\n",
        "- **Batch Gradient Descent**: Uses the entire dataset to compute the gradient, leading to stable but slow updates.\n",
        "- **Stochastic Gradient Descent (SGD)**: Uses a single data point per iteration, making updates faster but more volatile.\n",
        "- **Mini-Batch Gradient Descent**: Uses a small batch of data for each update, balancing stability and speed.\n",
        "\n",
        "---\n",
        "\n",
        "### **Usage in Machine Learning**\n",
        "- **Linear Regression**: Finds the best-fitting line by minimizing the mean squared error.\n",
        "- **Logistic Regression**: Optimizes the decision boundary for classification.\n",
        "- **Neural Networks**: Trains deep learning models by adjusting weights through backpropagation.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Gradient descent is a fundamental optimization technique that helps machine learning models improve their accuracy by minimizing the loss function efficiently.\n"
      ],
      "metadata": {
        "id": "0s-JlZFyeHae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
        "\n",
        "**Multiple Linear Regression (MLR)** is an extension of simple linear regression where a dependent variable is predicted using multiple independent variables. It models the relationship as:\n",
        "\n",
        "\\[\n",
        "Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n + \\epsilon\n",
        "\\]\n",
        "\n",
        "where:  \n",
        "- \\( Y \\) is the dependent variable.  \n",
        "- \\( X_1, X_2, ..., X_n \\) are independent variables (predictors).  \n",
        "- \\( \\beta_0 \\) is the intercept.  \n",
        "- \\( \\beta_1, \\beta_2, ..., \\beta_n \\) are coefficients that represent the effect of each predictor.  \n",
        "- \\( \\epsilon \\) is the error term.\n",
        "\n",
        "---\n",
        "\n",
        "### **Difference Between Simple and Multiple Linear Regression**\n",
        "| Feature               | Simple Linear Regression | Multiple Linear Regression |\n",
        "|-----------------------|------------------------|---------------------------|\n",
        "| Number of Predictors  | One independent variable | Two or more independent variables |\n",
        "| Equation Form         | \\( Y = \\beta_0 + \\beta_1X + \\epsilon \\) | \\( Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n + \\epsilon \\) |\n",
        "| Complexity            | Simple                  | More complex due to multiple predictors |\n",
        "| Use Case Example      | Predicting house price based on area | Predicting house price based on area, number of rooms, and location |\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Multiple Linear Regression is used when multiple factors influence the dependent variable, making it more powerful than simple linear regression but requiring careful handling of multicollinearity and feature selection.\n"
      ],
      "metadata": {
        "id": "UGmABFzmeZU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
        "\n",
        "## **What is Multicollinearity?**\n",
        "Multicollinearity occurs in **multiple linear regression** when two or more independent variables are highly correlated with each other. This makes it difficult for the model to determine the true effect of each predictor on the dependent variable.\n",
        "\n",
        "### **Why is Multicollinearity a Problem?**\n",
        "- It leads to **unstable coefficients**, making it hard to interpret the model.\n",
        "- It increases **variance** in coefficient estimates, reducing reliability.\n",
        "- It can cause **insignificant p-values**, even if predictors are important.\n",
        "\n",
        "---\n",
        "\n",
        "## **How to Detect Multicollinearity?**\n",
        "1. **Correlation Matrix**  \n",
        "   - Compute pairwise correlations between independent variables.\n",
        "   - High correlations (above **0.7 or 0.8**) indicate multicollinearity.\n",
        "\n",
        "2. **Variance Inflation Factor (VIF)**  \n",
        "   - Measures how much a variable is explained by other predictors.  \n",
        "   - A **VIF > 10** indicates strong multicollinearity.\n",
        "\n",
        "3. **Eigenvalues and Condition Index**  \n",
        "   - A high condition index (above **30**) suggests multicollinearity.\n",
        "\n",
        "---\n",
        "\n",
        "## **How to Address Multicollinearity?**\n",
        "1. **Remove Highly Correlated Variables**  \n",
        "   - Drop one of the correlated variables if they provide similar information.\n",
        "\n",
        "2. **Feature Engineering (Combine Variables)**  \n",
        "   - Create a single feature (e.g., **Principal Component Analysis (PCA)**).\n",
        "\n",
        "3. **Increase Sample Size**  \n",
        "   - More data can help reduce multicollinearity effects.\n",
        "\n",
        "4. **Use Regularization Techniques**  \n",
        "   - **Ridge Regression (L2 Regularization)** helps stabilize coefficients.\n",
        "\n",
        "---\n",
        "\n",
        "## **Conclusion**\n",
        "Multicollinearity can negatively impact model performance and interpretation. Detecting it with VIF or correlation matrices and addressing it using feature selection or regularization ensures a more robust regression model.\n"
      ],
      "metadata": {
        "id": "pFtH-3dbe4pA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
        "\n",
        "## **What is Polynomial Regression?**\n",
        "Polynomial regression is an extension of **linear regression** where the relationship between the independent variable (**X**) and the dependent variable (**Y**) is modeled as an **nth-degree polynomial** instead of a straight line.\n",
        "\n",
        "The general equation for polynomial regression is:\n",
        "\n",
        "\\[\n",
        "Y = b_0 + b_1X + b_2X^2 + b_3X^3 + ... + b_nX^n + \\epsilon\n",
        "\\]\n",
        "\n",
        "Here, **\\(X^2, X^3, ..., X^n\\)** introduce curvature to the model.\n",
        "\n",
        "---\n",
        "\n",
        "## **Difference Between Linear and Polynomial Regression**\n",
        "| Feature | Linear Regression | Polynomial Regression |\n",
        "|---------|------------------|----------------------|\n",
        "| **Equation** | \\( Y = b_0 + b_1X + \\epsilon \\) | \\( Y = b_0 + b_1X + b_2X^2 + ... + b_nX^n + \\epsilon \\) |\n",
        "| **Relationship** | Assumes a **linear** relationship between X and Y | Captures **non-linear** relationships |\n",
        "| **Curve Shape** | Straight line | Curved (parabolic, cubic, etc.) |\n",
        "| **Complexity** | Simple model | More flexible but can overfit |\n",
        "| **Interpretability** | Easy to interpret coefficients | Harder to interpret higher-degree terms |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "mKnzeZ6gfTGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
        "\n",
        "## **Advantages of Polynomial Regression**\n",
        "1. **Captures Non-Linear Relationships**  \n",
        "   - Unlike linear regression, it can model **curved** trends in data.\n",
        "2. **Better Fit for Complex Data**  \n",
        "   - When data does not follow a straight line, polynomial regression provides a more **accurate fit**.\n",
        "3. **More Flexibility**  \n",
        "   - By increasing the polynomial degree, we can adjust the model to fit various shapes in the data.\n",
        "\n",
        "---\n",
        "\n",
        "## **Disadvantages of Polynomial Regression**\n",
        "1. **Overfitting**  \n",
        "   - High-degree polynomials may fit training data well but **fail on new data**.\n",
        "2. **Increased Complexity**  \n",
        "   - Higher-degree models are harder to **interpret** and **compute**.\n",
        "3. **Sensitive to Outliers**  \n",
        "   - Small changes in data can **drastically** affect the curve, leading to **instability**.\n",
        "4. **Risk of Multicollinearity**  \n",
        "   - Polynomial terms (\\(X^2, X^3, ...\\)) can be highly correlated, making the model unstable.\n",
        "\n",
        "---\n",
        "\n",
        "## **When to Use Polynomial Regression?**\n",
        "- When a **linear model does not fit** the data well.\n",
        "- If the data shows a **non-linear** trend but remains **predictable**.\n",
        "- In scenarios like:\n",
        "  - **Stock price trends**\n",
        "  - **Growth rate predictions**\n",
        "  - **Scientific experiments (e.g., physics, biology models)**\n",
        "  - **Marketing & sales forecasting**\n",
        "\n",
        "---\n",
        "\n",
        "## **Conclusion**\n",
        "Polynomial regression is useful when dealing with **non-linear patterns**, but it requires careful tuning to **avoid overfitting**. If the relationship is strongly curved, **low-degree polynomials (e.g., quadratic or cubic)** may be a better choice than using very high-degree polynomials.\n"
      ],
      "metadata": {
        "id": "Hca7DHA8f_CI"
      }
    }
  ]
}