{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
        "\n",
        "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a **linear regression technique** that adds **L1 regularization** to the loss function. This penalizes the absolute sum of coefficients, effectively shrinking some of them to **zero**, which enables **automatic feature selection**.\n",
        "\n",
        "### Differences from Other Regression Techniques:\n",
        "1. **Ordinary Least Squares (OLS):** OLS does not include regularization, which can lead to overfitting when features are highly correlated.\n",
        "2. **Ridge Regression:** Ridge applies **L2 regularization**, shrinking coefficients toward zero but never setting them to exactly zero, meaning all features are retained.\n",
        "3. **Elastic Net Regression:** Elastic Net combines both L1 (Lasso) and L2 (Ridge) penalties, balancing feature selection and coefficient shrinkage.\n",
        "\n",
        "Lasso is particularly useful when working with **high-dimensional datasets** and **sparse models**, as it helps identify the most important predictors by eliminating irrelevant features.\n"
      ],
      "metadata": {
        "id": "gsGpXtuRqEqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
        "\n",
        "The main advantage of **Lasso Regression** in feature selection is its ability to **automatically eliminate irrelevant or less important features** by shrinking their coefficients to **exactly zero**. This leads to a **sparser model**, improving interpretability and reducing overfitting.\n",
        "\n",
        "### Key Benefits:\n",
        "- **Feature Selection:** Unimportant features get removed, simplifying the model.\n",
        "- **Better Interpretability:** Only the most relevant predictors remain.\n",
        "- **Reduced Multicollinearity:** Helps in handling correlated variables by selecting one and ignoring the others.\n",
        "- **Improved Model Efficiency:** With fewer features, the model trains faster and generalizes better.\n",
        "\n",
        "This makes Lasso Regression especially useful for **high-dimensional datasets** where feature selection is crucial.\n"
      ],
      "metadata": {
        "id": "PgDcJEd1vlKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
        "\n",
        "In **Lasso Regression**, the coefficients represent the relationship between each independent variable and the dependent variable, similar to **Ordinary Least Squares (OLS) regression**. However, due to the **L1 regularization penalty**, some coefficients may shrink to **exactly zero**, effectively removing those features from the model.\n",
        "\n",
        "### Interpretation:\n",
        "1. **Non-Zero Coefficients:** These indicate that the corresponding features contribute to the prediction.\n",
        "2. **Zero Coefficients:** These features are completely removed from the model, meaning they are not significant in predicting the target variable.\n",
        "3. **Magnitude of Coefficients:** Larger absolute values suggest a stronger influence on the dependent variable.\n",
        "\n",
        "### Example:\n",
        "If we apply Lasso to predict house prices:\n",
        "- A coefficient of **50,000** for `square_footage` means that increasing the house size by 1 unit increases the price by **₹50,000**, assuming other factors remain constant.\n",
        "- A coefficient of **0** for `garden_size` means that `garden_size` has no significant impact on house price and has been removed by Lasso.\n",
        "\n",
        "This ability to **select important features** makes Lasso a powerful tool for both **prediction and feature selection**.\n"
      ],
      "metadata": {
        "id": "b8IxAB15v3T2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
        "\n",
        "### **Tuning Parameter in Lasso Regression:**\n",
        "The main tuning parameter in **Lasso Regression** is **lambda (α)**, which controls the strength of the **L1 regularization**. This parameter determines how much penalty is applied to the model coefficients.\n",
        "\n",
        "### **Effect of Lambda (α) on Model Performance:**\n",
        "1. **Small α (Close to 0)**:\n",
        "   - **Minimal regularization**, making Lasso behave like **Ordinary Least Squares (OLS)**.\n",
        "   - **More features are retained**, leading to a **complex model**.\n",
        "   - **Higher risk of overfitting**.\n",
        "\n",
        "2. **Moderate α**:\n",
        "   - **Some coefficients shrink to zero**, performing **feature selection**.\n",
        "   - **Balances model complexity and performance**.\n",
        "   - **Reduces overfitting** while maintaining relevant features.\n",
        "\n",
        "3. **Large α**:\n",
        "   - **Strong regularization**, shrinking more coefficients to **exactly zero**.\n",
        "   - **Fewer features are used**, leading to a **simpler model**.\n",
        "   - **May underfit the data** if too many important features are removed.\n",
        "\n",
        "### **Tuning Lambda (α):**\n",
        "To find the optimal **α**, we can use:\n",
        "- **Cross-validation (GridSearchCV or RandomizedSearchCV)**\n",
        "- **Regularization paths (e.g., LassoCV in Scikit-learn)**\n",
        "\n",
        "### **Other Parameters in Lasso:**\n",
        "- `max_iter`: Controls the number of iterations for optimization.\n",
        "- `tol`: Determines convergence criteria.\n",
        "- `fit_intercept`: Whether to include an intercept term.\n",
        "\n",
        "### **Conclusion:**\n",
        "Tuning **λ (alpha)** is crucial for balancing **bias-variance tradeoff**. Choosing an optimal value ensures **good generalization**, avoiding both **overfitting and underfitting**.\n"
      ],
      "metadata": {
        "id": "NctJjzz0wIPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
        "\n",
        "### **Lasso Regression and Non-Linearity**\n",
        "Lasso Regression is a **linear model** by default, meaning it assumes a **linear relationship** between independent variables (features) and the target variable. However, it can be extended to handle **non-linear regression problems** by transforming the input features.\n",
        "\n",
        "### **How to Use Lasso for Non-Linear Regression?**\n",
        "1. **Feature Engineering with Polynomial Features**  \n",
        "   - Convert the original features into **polynomial terms** (e.g., quadratic, cubic).\n",
        "   - Example: If `X` is the original feature, transform it into `X²`, `X³`, etc.\n",
        "   - Use **PolynomialFeatures** from `sklearn.preprocessing` to generate non-linear terms.\n",
        "\n",
        "2. **Apply Lasso Regression on Transformed Features**  \n",
        "   - After transforming the data into a higher-dimensional space, Lasso can be used to **select the most important features** while preventing overfitting."
      ],
      "metadata": {
        "id": "Mj3foB_4wYFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
        "\n",
        "### **Key Differences Between Ridge and Lasso Regression**\n",
        "| Feature            | Ridge Regression | Lasso Regression |\n",
        "|-------------------|----------------|----------------|\n",
        "| **Regularization Type** | L2 (squared sum of coefficients) | L1 (absolute sum of coefficients) |\n",
        "| **Effect on Coefficients** | Shrinks coefficients but does not set them to zero | Shrinks coefficients and can set some to zero (feature selection) |\n",
        "| **Feature Selection** | No feature selection (keeps all features) | Performs feature selection (eliminates less important features) |\n",
        "| **Best Used When** | All features are important, and multicollinearity is present | Some features are irrelevant, and automatic feature selection is needed |\n",
        "| **Computational Complexity** | Slightly more efficient than Lasso | Can be computationally expensive when many features are zeroed out |\n",
        "| **Handling Multicollinearity** | Reduces multicollinearity by shrinking correlated coefficients | Also reduces multicollinearity but may eliminate some correlated features |\n"
      ],
      "metadata": {
        "id": "STmLCpKdxDJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
        "\n",
        "### **Yes, Lasso Regression can handle multicollinearity, but with limitations.**\n",
        "\n",
        "### **How Lasso Handles Multicollinearity**\n",
        "1. **Feature Selection:** Lasso applies L1 regularization, which forces some feature coefficients to become exactly zero. This helps remove redundant or highly correlated features, reducing multicollinearity.\n",
        "2. **Sparse Model:** By eliminating some features, Lasso selects the most relevant ones, preventing overfitting caused by multicollinearity.\n",
        "3. **Bias-Variance Tradeoff:** Lasso introduces bias to the model, reducing variance and making it more robust in the presence of correlated features.\n",
        "\n",
        "### **Limitations**\n",
        "- **Arbitrary Feature Selection:** If two or more features are highly correlated, Lasso may randomly select one and eliminate the others, which can lead to loss of valuable information.\n",
        "- **Not Ideal for Highly Correlated Data:** Ridge regression is often preferred when dealing with extreme multicollinearity because it shrinks coefficients without setting them to zero.\n",
        "\n",
        "### **Conclusion**\n",
        "Lasso can handle multicollinearity by performing feature selection, but it may arbitrarily drop correlated features. If retaining all correlated features is important, **Ridge Regression** might be a better choice.\n"
      ],
      "metadata": {
        "id": "gSU-c96txYm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
        "\n",
        "### **The optimal value of lambda in Lasso Regression is chosen using cross-validation.**\n",
        "\n",
        "### **Methods to Select Lambda (λ)**\n",
        "1. **Cross-Validation (CV):**  \n",
        "   - Perform **k-fold cross-validation** (commonly 5-fold or 10-fold) on the training data.\n",
        "   - Choose the λ value that minimizes the validation error (e.g., Mean Squared Error).\n",
        "   - This ensures a balance between underfitting and overfitting.\n",
        "\n",
        "2. **Grid Search:**  \n",
        "   - Define a range of λ values (e.g., from `0.001` to `10`).\n",
        "   - Train the model on different λ values and evaluate performance.\n",
        "   - Select the λ that gives the best prediction accuracy.\n",
        "\n",
        "3. **Lasso Path (Using LARS Algorithm):**  \n",
        "   - Compute the entire path of solutions for different λ values.\n",
        "   - Use **Akaike Information Criterion (AIC)** or **Bayesian Information Criterion (BIC)** to choose the best λ.\n",
        "\n",
        "4. **Regularization Path Visualization:**  \n",
        "   - Plot feature coefficients vs. λ values.\n",
        "   - Identify the point where important features remain while unimportant ones shrink to zero.\n",
        "\n",
        "### **Conclusion**\n",
        "Selecting λ using **cross-validation** is the most common and effective method. A small λ leads to minimal regularization (risking overfitting), while a large λ results in strong regularization (risking underfitting). The optimal λ balances these effects.\n"
      ],
      "metadata": {
        "id": "CGSLS02RxkKQ"
      }
    }
  ]
}