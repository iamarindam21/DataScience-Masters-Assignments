{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. Explain the concept of precision and recall in the context of classification models.\n",
        "\n",
        "Precision and recall are key evaluation metrics for classification models, especially in imbalanced datasets.\n",
        "\n",
        "## **1. Precision (Positive Predictive Value)**\n",
        "- Measures how many of the predicted positive cases are actually positive.\n",
        "- Formula:  \n",
        "  \\[\n",
        "  \\text{Precision} = \\frac{TP}{TP + FP}\n",
        "  \\]\n",
        "- **Interpretation:**  \n",
        "  - High precision means fewer false positives.\n",
        "  - Useful when **false positives** are costly (e.g., spam detection, fraud detection).\n",
        "\n",
        "## **2. Recall (Sensitivity or True Positive Rate)**\n",
        "- Measures how many actual positive cases were correctly identified.\n",
        "- Formula:  \n",
        "  \\[\n",
        "  \\text{Recall} = \\frac{TP}{TP + FN}\n",
        "  \\]\n",
        "- **Interpretation:**  \n",
        "  - High recall means fewer false negatives.\n",
        "  - Useful when **false negatives** are costly (e.g., medical diagnosis, fraud detection).\n",
        "\n",
        "## **3. Precision vs. Recall Trade-off**\n",
        "- A model with high precision may have low recall and vice versa.\n",
        "- The **F1-score** is often used to balance precision and recall.\n",
        "\n",
        "### **Example Scenario: Medical Diagnosis**\n",
        "- **High Precision:** Ensures that when the model predicts a disease, it is correct, avoiding unnecessary panic or treatments.\n",
        "- **High Recall:** Ensures all diseased patients are detected, even if some false positives occur.\n",
        "\n",
        "Choosing between precision and recall depends on the specific problem and the cost of false positives vs. false negatives.\n"
      ],
      "metadata": {
        "id": "PerOzBSNGJ6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. What is the F1 score and how is it calculated? How is it different from precision and recall?\n",
        "\n",
        "## **1. What is the F1 Score?**\n",
        "The **F1 score** is the harmonic mean of precision and recall. It provides a single metric that balances the trade-off between precision and recall.\n",
        "\n",
        "## **2. Formula for F1 Score**\n",
        "\\[\n",
        "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "\\]\n",
        "Where:  \n",
        "- **Precision** = \\( \\frac{TP}{TP + FP} \\)  \n",
        "- **Recall** = \\( \\frac{TP}{TP + FN} \\)  \n",
        "\n",
        "## **3. Difference Between F1 Score, Precision, and Recall**\n",
        "- **Precision** measures how many predicted positives are actually correct.\n",
        "- **Recall** measures how many actual positives were correctly identified.\n",
        "- **F1 Score** balances both by penalizing extreme values of either metric.\n",
        "\n",
        "## **4. Why Use F1 Score?**\n",
        "- Useful when dealing with **imbalanced datasets**.\n",
        "- Ensures that both **false positives (FP)** and **false negatives (FN)** are considered.\n",
        "- Helps in situations where optimizing precision or recall alone is not enough.\n",
        "\n",
        "## **5. Example Scenario: Fraud Detection**\n",
        "- If a model has **high precision but low recall**, it misses too many fraud cases.\n",
        "- If a model has **high recall but low precision**, it flags too many legitimate transactions as fraud.\n",
        "- **F1 Score** provides a balance, ensuring both fraud cases are caught while minimizing false alarms.\n"
      ],
      "metadata": {
        "id": "ADearC0lGOj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. What is ROC and AUC, and how are they used to evaluate the performance of classification models?\n",
        "\n",
        "## **1. What is ROC (Receiver Operating Characteristic) Curve?**\n",
        "The **ROC curve** is a graphical representation of a classification model's performance across different threshold values. It plots:\n",
        "\n",
        "- **True Positive Rate (TPR) or Sensitivity (Recall)** on the Y-axis:  \n",
        "  \\[\n",
        "  TPR = \\frac{TP}{TP + FN}\n",
        "  \\]\n",
        "- **False Positive Rate (FPR)** on the X-axis:  \n",
        "  \\[\n",
        "  FPR = \\frac{FP}{FP + TN}\n",
        "  \\]\n",
        "\n",
        "A perfect model has a curve that reaches the top-left corner (TPR = 1, FPR = 0).\n",
        "\n",
        "## **2. What is AUC (Area Under the Curve)?**\n",
        "- **AUC** is the area under the ROC curve.\n",
        "- It measures the **overall ability** of the model to distinguish between positive and negative classes.\n",
        "- **AUC values range from 0 to 1**:\n",
        "  - **AUC = 1** → Perfect classification.\n",
        "  - **AUC = 0.5** → Random guessing (no discrimination).\n",
        "  - **AUC < 0.5** → Worse than random.\n",
        "\n",
        "## **3. How Are ROC and AUC Used for Model Evaluation?**\n",
        "- **Comparing Models:** Higher AUC values indicate better performance.\n",
        "- **Choosing Thresholds:** Helps select the best probability threshold for classification.\n",
        "- **Imbalanced Data:** AUC-ROC is useful when dealing with imbalanced datasets since it considers both TPR and FPR.\n",
        "\n",
        "## **4. Example Use Case: Medical Diagnosis**\n",
        "- **High AUC (near 1.0):** Model effectively differentiates between patients with and without a disease.\n",
        "- **Low AUC (near 0.5):** Model is no better than random chance.\n",
        "\n",
        "Thus, **ROC and AUC provide a comprehensive way to evaluate a classifier’s performance beyond just accuracy**.\n"
      ],
      "metadata": {
        "id": "SN9EuntyGi7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. How do you choose the best metric to evaluate the performance of a classification model?  What is Multiclass Classification and How is it Different from Binary Classification?\n",
        "\n",
        "## **1. Factors to Consider When Choosing a Metric**\n",
        "Selecting the right evaluation metric depends on:\n",
        "- **The type of classification problem** (binary, multi-class, or imbalanced).\n",
        "- **The importance of false positives vs. false negatives** in the application.\n",
        "- **The dataset's balance** (equal or imbalanced class distribution).\n",
        "\n",
        "## **2. Common Evaluation Metrics and When to Use Them**\n",
        "| **Metric**         | **Formula** | **When to Use** |\n",
        "|--------------------|------------|----------------|\n",
        "| **Accuracy**       | \\( \\frac{TP + TN}{TP + TN + FP + FN} \\) | When class distribution is balanced. |\n",
        "| **Precision**      | \\( \\frac{TP}{TP + FP} \\) | When false positives are costly (e.g., spam detection). |\n",
        "| **Recall (Sensitivity)** | \\( \\frac{TP}{TP + FN} \\) | When false negatives are costly (e.g., medical diagnosis). |\n",
        "| **F1 Score**       | \\( 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} \\) | When both false positives and false negatives matter. |\n",
        "| **ROC-AUC Score**  | Area under ROC curve | When evaluating overall model discrimination ability. |\n",
        "| **PR-AUC Score**   | Area under Precision-Recall curve | When dealing with highly imbalanced datasets. |\n",
        "\n",
        "## **3. Choosing the Best Metric Based on the Use Case**\n",
        "- **Spam Detection:** **Precision** is crucial to avoid flagging important emails as spam.\n",
        "- **Medical Diagnosis:** **Recall** is important to minimize missing true positive cases.\n",
        "- **Fraud Detection:** **F1-score** balances precision and recall for a fair evaluation.\n",
        "- **Imbalanced Datasets:** **ROC-AUC or PR-AUC** are better than accuracy.\n",
        "\n",
        "### **Conclusion**\n",
        "The best metric depends on the problem. If **false positives and false negatives have different consequences**, precision, recall, or F1-score should be prioritized over accuracy.\n",
        "\n",
        "\n",
        "\n",
        "## **1. Definition of Multiclass Classification**\n",
        "Multiclass classification is a type of classification problem where a model predicts one class label from **three or more possible classes**. Each instance belongs to exactly one category.\n",
        "\n",
        "### **Example:**\n",
        "- Classifying types of animals: **Dog, Cat, Bird, or Fish**.\n",
        "- Predicting customer sentiment: **Positive, Neutral, or Negative**.\n",
        "\n",
        "## **2. Difference Between Multiclass and Binary Classification**\n",
        "| **Aspect**            | **Binary Classification**           | **Multiclass Classification** |\n",
        "|----------------------|---------------------------------|-----------------------------|\n",
        "| **Number of Classes** | Only **two** classes (e.g., 0 or 1, Yes or No). | **Three or more** distinct classes. |\n",
        "| **Output Labels** | The model predicts **one of two** possible labels. | The model predicts **one of multiple** possible labels. |\n",
        "| **Example** | Spam detection (**Spam or Not Spam**). | Classifying types of fruits (**Apple, Banana, Orange**). |\n",
        "| **Common Algorithms** | Logistic Regression, SVM, Decision Trees. | Softmax-based Logistic Regression, Random Forest, Neural Networks. |\n",
        "| **Evaluation Metrics** | Accuracy, Precision, Recall, F1-Score, ROC-AUC. | Accuracy, Macro/Micro-Averaged Precision, Recall, F1-Score. |\n",
        "\n",
        "## **3. Key Differences in Model Approach**\n",
        "- **Binary Classification**: Uses **sigmoid activation** to output a probability between **0 and 1**.\n",
        "- **Multiclass Classification**: Uses **softmax activation**, assigning probabilities to each class and selecting the class with the **highest probability**.\n",
        "\n",
        "## **4. Common Techniques for Multiclass Classification**\n",
        "- **One-vs-All (OvA)**: Trains multiple binary classifiers, one for each class.\n",
        "- **One-vs-One (OvO)**: Trains classifiers for every possible pair of classes.\n",
        "- **Softmax Regression**: Directly predicts probabilities for all classes in a single model.\n",
        "\n",
        "### **Conclusion**\n",
        "Multiclass classification is an extension of binary classification, handling scenarios with more than two class labels. Choosing the right algorithm and evaluation metric depends on the dataset and problem requirements.\n"
      ],
      "metadata": {
        "id": "DmXfc1-NGyIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. Explain How Logistic Regression Can Be Used for Multiclass Classification\n",
        "\n",
        "## **1. Logistic Regression for Binary Classification**\n",
        "Logistic Regression is commonly used for **binary classification**, where the output is either **0 or 1**. It applies the **sigmoid function** to predict probabilities.\n",
        "\n",
        "## **2. Extending Logistic Regression to Multiclass Classification**\n",
        "For **multiclass classification (three or more classes)**, logistic regression is extended using the following approaches:\n",
        "\n",
        "### **A. One-vs-All (OvA) / One-vs-Rest (OvR) Approach**\n",
        "- **Concept**: Converts a multiclass problem into multiple **binary classification** problems.\n",
        "- **How it works**:\n",
        "  1. For each class, train a separate logistic regression model.\n",
        "  2. The model treats the current class as **\"1\" (positive)** and all others as **\"0\" (negative)**.\n",
        "  3. After training, the class with the **highest probability** is chosen as the final prediction.\n",
        "- **Example**:\n",
        "  - Classes: **Apple, Banana, Orange**.\n",
        "  - Three models:\n",
        "    - Apple vs. (Banana + Orange)\n",
        "    - Banana vs. (Apple + Orange)\n",
        "    - Orange vs. (Apple + Banana)\n",
        "\n",
        "### **B. One-vs-One (OvO) Approach**\n",
        "- **Concept**: Trains **binary classifiers for every possible pair** of classes.\n",
        "- **How it works**:\n",
        "  1. If there are **N** classes, train **N*(N-1)/2** classifiers.\n",
        "  2. Each classifier differentiates between two classes.\n",
        "  3. The final class is determined using **majority voting**.\n",
        "\n",
        "### **C. Softmax Regression (Multinomial Logistic Regression)**\n",
        "- **Concept**: Extends logistic regression by using the **softmax function** instead of the sigmoid function.\n",
        "- **How it works**:\n",
        "  1. Instead of predicting **binary probabilities**, softmax assigns a probability to each class.\n",
        "  2. The class with the **highest probability** is the final prediction.\n",
        "- **Mathematical Formula**:\n",
        "  \\[\n",
        "  P(y = c) = \\frac{e^{\\theta_c^T x}}{\\sum_{j=1}^{N} e^{\\theta_j^T x}}\n",
        "  \\]\n",
        "  Where:\n",
        "  - \\( P(y = c) \\) is the probability of class **c**.\n",
        "  - \\( \\theta_c \\) represents the coefficients for class **c**.\n",
        "  - \\( x \\) is the input features.\n",
        "\n",
        "## **3. Choosing the Right Approach**\n",
        "| **Method**   | **Advantages** | **Disadvantages** |\n",
        "|-------------|---------------|------------------|\n",
        "| **OvA (One-vs-All)** | Simple, fast for large datasets. | Can be biased if classes are imbalanced. |\n",
        "| **OvO (One-vs-One)** | Works well for small datasets. | Computationally expensive for large datasets. |\n",
        "| **Softmax Regression** | Efficient, handles multiple classes in one model. | Requires more complex optimization. |\n",
        "\n",
        "## **Conclusion**\n",
        "Logistic regression can be used for **multiclass classification** using **One-vs-All (OvA), One-vs-One (OvO), or Softmax Regression**. The choice depends on dataset size, complexity, and computational efficiency.\n"
      ],
      "metadata": {
        "id": "B90jvAhHHmCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6. Describe the Steps Involved in an End-to-End Project for Multiclass Classification\n",
        "\n",
        "## **1. Problem Definition**\n",
        "- Clearly define the objective of the classification task.\n",
        "- Example: Classify different types of flowers (Setosa, Versicolor, Virginica).\n",
        "\n",
        "## **2. Data Collection**\n",
        "- Gather relevant data from available sources (CSV files, databases, APIs).\n",
        "- Ensure the dataset contains labeled examples for each class.\n",
        "\n",
        "## **3. Data Preprocessing**\n",
        "- **Handling Missing Values**: Fill or drop missing data.\n",
        "- **Feature Engineering**: Create new features if necessary.\n",
        "- **Data Normalization/Scaling**: Standardize numerical features to improve model performance.\n",
        "- **Encoding Categorical Data**: Convert categorical variables into numerical form (One-Hot Encoding, Label Encoding).\n",
        "\n",
        "## **4. Exploratory Data Analysis (EDA)**\n",
        "- Visualize class distribution using bar plots.\n",
        "- Check for correlations using heatmaps.\n",
        "- Identify outliers and remove if necessary.\n",
        "\n",
        "## **5. Data Splitting**\n",
        "- Split the dataset into:\n",
        "  - **Training set** (e.g., 70%)\n",
        "  - **Validation set** (e.g., 15%)\n",
        "  - **Test set** (e.g., 15%)\n",
        "\n",
        "## **6. Model Selection**\n",
        "- Choose an appropriate classification algorithm:\n",
        "  - Logistic Regression (Softmax)\n",
        "  - Decision Trees\n",
        "  - Random Forest\n",
        "  - Support Vector Machines (SVM)\n",
        "  - Neural Networks\n",
        "\n",
        "## **7. Model Training**\n",
        "- Train the selected model using the training dataset.\n",
        "- Optimize hyperparameters using techniques like **Grid Search CV** or **Random Search CV**.\n",
        "\n",
        "## **8. Model Evaluation**\n",
        "- Use metrics such as:\n",
        "  - **Accuracy**: Overall correctness of predictions.\n",
        "  - **Precision, Recall, and F1-score**: Evaluate per-class performance.\n",
        "  - **Confusion Matrix**: Identify misclassifications.\n",
        "  - **ROC-AUC Score**: Measure model discrimination ability.\n",
        "\n",
        "## **9. Hyperparameter Tuning**\n",
        "- Optimize model parameters using:\n",
        "  - **Grid Search CV**\n",
        "  - **Random Search CV**\n",
        "  - **Bayesian Optimization**\n",
        "\n",
        "## **10. Model Deployment**\n",
        "- Convert the trained model into a deployable format using:\n",
        "  - **Pickle (.pkl)**\n",
        "  - **Joblib**\n",
        "- Deploy the model using:\n",
        "  - **Flask/Django for Web APIs**\n",
        "  - **FastAPI for high-performance applications**\n",
        "  - **Cloud Services (AWS, GCP, Azure)**\n",
        "\n",
        "## **11. Model Monitoring and Maintenance**\n",
        "- Track model performance over time.\n",
        "- Retrain the model periodically with new data.\n",
        "- Implement logging and alert mechanisms for performance degradation.\n",
        "\n",
        "## **Conclusion**\n",
        "By following these steps, an end-to-end multiclass classification project can be effectively implemented, deployed, and maintained for real-world applications.\n"
      ],
      "metadata": {
        "id": "UPtWvmMzIB8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q7. What is Model Deployment and Why is it Important?\n",
        "\n",
        "## **What is Model Deployment?**\n",
        "Model deployment is the process of integrating a trained machine learning model into a real-world environment where it can make predictions on new data. This involves serving the model via APIs, embedding it into applications, or deploying it on cloud platforms.\n",
        "\n",
        "## **Why is Model Deployment Important?**\n",
        "1. **Real-World Usage**  \n",
        "   - Allows businesses and users to make predictions using the trained model.\n",
        "   \n",
        "2. **Automation of Decision-Making**  \n",
        "   - Enables automated systems to make data-driven decisions without human intervention.\n",
        "   \n",
        "3. **Scalability**  \n",
        "   - Deployed models can handle large volumes of real-time data and requests efficiently.\n",
        "   \n",
        "4. **Improved Accessibility**  \n",
        "   - Makes the model accessible through APIs, web applications, or mobile apps.\n",
        "   \n",
        "5. **Continuous Improvement**  \n",
        "   - Allows retraining and updating models based on real-world feedback.\n",
        "   \n",
        "6. **Business Impact**  \n",
        "   - Supports better decision-making, customer engagement, and operational efficiency.\n",
        "\n",
        "## **Common Model Deployment Methods**\n",
        "- **Local Deployment**: Running the model on a personal or enterprise server.\n",
        "- **Web API Deployment**: Using frameworks like Flask, FastAPI, or Django to serve predictions.\n",
        "- **Cloud Deployment**: Hosting on AWS, GCP, or Azure for scalability.\n",
        "- **Edge Deployment**: Deploying on IoT devices for real-time predictions.\n",
        "\n",
        "Model deployment is a critical step in machine learning workflows, ensuring that models provide real-world value rather than just theoretical results.\n"
      ],
      "metadata": {
        "id": "btz1tp0EIeSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q8. Explain How Multi-Cloud Platforms Are Used for Model Deployment\n",
        "\n",
        "## **What Are Multi-Cloud Platforms?**  \n",
        "Multi-cloud platforms involve using multiple cloud service providers (such as AWS, Google Cloud, and Azure) to deploy and manage machine learning models. This approach enhances flexibility, reliability, and scalability.\n",
        "\n",
        "## **How Multi-Cloud Platforms Are Used for Model Deployment?**  \n",
        "1. **Cross-Cloud Deployment**  \n",
        "   - Deploying models across multiple cloud providers to avoid vendor lock-in.  \n",
        "   - Example: Training a model on Google Cloud AI Platform but serving it via AWS Lambda.\n",
        "\n",
        "2. **Load Balancing and Redundancy**  \n",
        "   - Distributing model inference requests across multiple clouds to ensure high availability.  \n",
        "   - If one cloud provider fails, another can take over, preventing downtime.\n",
        "\n",
        "3. **Optimized Resource Utilization**  \n",
        "   - Selecting cloud services based on cost, performance, and availability.  \n",
        "   - Example: Using GPU instances from Google Cloud for training but deploying inference models on Azure.\n",
        "\n",
        "4. **Compliance and Data Localization**  \n",
        "   - Ensuring data processing complies with regional regulations by using specific cloud providers in different locations.  \n",
        "   - Example: Deploying models in AWS Europe for GDPR compliance while using GCP in the U.S.\n",
        "\n",
        "5. **Hybrid Model Deployment**  \n",
        "   - Combining on-premise servers with multiple cloud platforms for flexible model serving.  \n",
        "   - Example: Running inference on-premises while storing model artifacts in the cloud.\n",
        "\n",
        "6. **Cloud-Native AI Services**  \n",
        "   - Using cloud-based AI services from different providers for efficiency.  \n",
        "   - Example: Deploying TensorFlow models on Google Cloud AI and PyTorch models on AWS SageMaker.\n",
        "\n"
      ],
      "metadata": {
        "id": "XGUKk9h7I1Qa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q9. Discuss the Benefits and Challenges of Deploying Machine Learning Models in a Multi-Cloud Environment\n",
        "\n",
        "## **Benefits of Multi-Cloud Model Deployment**  \n",
        "\n",
        "1. **Avoids Vendor Lock-In**  \n",
        "   - Allows flexibility in choosing cloud providers based on cost, performance, and features.  \n",
        "   - Prevents dependency on a single provider's infrastructure and policies.  \n",
        "\n",
        "2. **Improved Reliability and Fault Tolerance**  \n",
        "   - Distributes workloads across multiple clouds to reduce the risk of downtime.  \n",
        "   - Ensures availability even if one cloud provider experiences failures.  \n",
        "\n",
        "3. **Optimized Cost and Performance**  \n",
        "   - Enables organizations to select the most cost-effective cloud services for different tasks.  \n",
        "   - Example: Using AWS for inference while leveraging Google Cloud’s TPU for model training.  \n",
        "\n",
        "4. **Scalability and Flexibility**  \n",
        "   - Allows dynamic scaling of machine learning models based on traffic demand.  \n",
        "   - Provides access to specialized AI services from different providers.  \n",
        "\n",
        "5. **Compliance and Data Sovereignty**  \n",
        "   - Helps meet regulatory requirements by storing and processing data in specific geographic regions.  \n",
        "   - Example: Using AWS for U.S. customers and Azure for European clients to comply with GDPR.  \n",
        "\n",
        "6. **Leveraging Best-of-Breed Services**  \n",
        "   - Organizations can combine different cloud providers' strengths.  \n",
        "   - Example: Using Google Cloud AI for NLP tasks and AWS SageMaker for general model deployment.  \n",
        "\n",
        "## **Challenges of Multi-Cloud Model Deployment**  \n",
        "\n",
        "1. **Increased Complexity**  \n",
        "   - Managing multiple cloud environments requires additional expertise and tools.  \n",
        "   - Requires setting up interoperability between different platforms.  \n",
        "\n",
        "2. **Higher Operational Costs**  \n",
        "   - Multi-cloud deployments may lead to increased costs due to data transfer fees and maintenance.  \n",
        "   - Requires investment in cloud monitoring and management solutions.  \n",
        "\n",
        "3. **Security and Compliance Risks**  \n",
        "   - Ensuring data security across multiple providers can be challenging.  \n",
        "   - Requires consistent security policies and encryption strategies.  \n",
        "\n",
        "4. **Latency and Performance Variability**  \n",
        "   - Differences in cloud infrastructure may affect model inference speed.  \n",
        "   - Requires optimizing data routing to minimize latency.  \n",
        "\n",
        "5. **Difficult Integration and Monitoring**  \n",
        "   - Deploying models across different cloud providers requires robust monitoring solutions.  \n",
        "   - Example: Logging and debugging across AWS, Azure, and GCP may require additional tools.  \n",
        "\n",
        "6. **Data Transfer and Synchronization Issues**  \n",
        "   - Moving large datasets between clouds may be costly and slow.  \n",
        "   - Requires efficient data synchronization strategies to maintain consistency.  \n",
        "\n",
        "### **Conclusion**  \n",
        "Deploying machine learning models in a multi-cloud environment offers flexibility, cost optimization, and improved reliability. However, organizations must address complexity, security, and integration challenges to maximize the benefits.\n"
      ],
      "metadata": {
        "id": "dPxUHXNiJMzU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "02sIVT-AJbmH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}