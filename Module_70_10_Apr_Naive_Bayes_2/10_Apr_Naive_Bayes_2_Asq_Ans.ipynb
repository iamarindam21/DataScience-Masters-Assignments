{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Q1. A company conducted a survey of its employees and found that 70% of the employees use the 
company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the 
probability that an employee is a smoker given that he/she uses the health insurance plan?**\n",
        "\n",
        "A company conducted a survey and found that:  \n",
        "- **70% of employees** use the company's health insurance plan.  \n",
        "- **40% of employees** who use the plan are **smokers**.\n",
        "\n",
        "We need to find the probability that an employee is a **smoker given that he/she uses the health insurance plan**.\n",
        "\n",
        "Using **Bayes' Theorem**, the conditional probability is given by:\n",
        "\n",
        "\\[\n",
        "P(\\text{Smoker} \\mid \\text{Uses Plan}) = P(\\text{Uses Plan} \\cap \\text{Smoker}) \\div P(\\text{Uses Plan})\n",
        "\\]\n",
        "\n",
        "From the data:  \n",
        "- \\( P(\\text{Smoker} \\mid \\text{Uses Plan}) = 0.40 \\)  \n",
        "- \\( P(\\text{Uses Plan}) = 0.70 \\)\n",
        "\n",
        "Thus, the probability that an employee is a **smoker given that they use the health insurance plan** is:\n",
        "\n",
        "\\[\n",
        "P(\\text{Smoker} \\mid \\text{Uses Plan}) = 0.40 \\text{ or } 40\\%\n",
        "\\]\n"
      ],
      "metadata": {
        "id": "ZiINWsjjMVup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q2. Difference Between Bernoulli Naive Bayes and Multinomial Naive Bayes**\n",
        "\n",
        "### **1. Bernoulli Naive Bayes**\n",
        "- Used for **binary/Boolean features** (0 or 1).\n",
        "- Assumes that each feature follows a **Bernoulli distribution** (i.e., presence or absence of a feature matters).\n",
        "- Commonly used for **text classification** tasks where features represent word presence/absence (e.g., spam detection).\n",
        "\n",
        "**Example:**  \n",
        "In a spam email classifier, if the presence of the word \"discount\" is considered (1 = present, 0 = absent).\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Multinomial Naive Bayes**\n",
        "- Used for **count-based features** (integer values).\n",
        "- Assumes that each feature follows a **Multinomial distribution** (i.e., feature frequency matters).\n",
        "- Commonly used for **text classification** where word frequency is important (e.g., document classification).\n",
        "\n",
        "**Example:**  \n",
        "In a topic classifier, the number of times the word \"discount\" appears in a document influences the classification.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Differences**\n",
        "| Feature                  | Bernoulli Naive Bayes | Multinomial Naive Bayes |\n",
        "|--------------------------|----------------------|-------------------------|\n",
        "| Data Type               | Binary (0 or 1)      | Count-based (0, 1, 2, …) |\n",
        "| Feature Representation  | Word presence/absence | Word frequency |\n",
        "| Common Use Cases       | Spam detection, sentiment analysis | Document classification, topic modeling |\n",
        "| Assumption             | Binary occurrence of features | Frequency-based occurrence of features |\n",
        "\n",
        "---\n",
        "### **Conclusion**\n",
        "- Use will use **Bernoulli Naive Bayes** when features are binary (e.g., presence/absence of words).  \n",
        "- Use will use **Multinomial Naive Bayes** when features represent counts (e.g., word frequency in documents).\n"
      ],
      "metadata": {
        "id": "RCZApSYkNQWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q3. How Does Bernoulli Naive Bayes Handle Missing Values?**\n",
        "\n",
        "### **1. Default Handling**\n",
        "- Bernoulli Naive Bayes does not have a built-in mechanism for handling missing values.\n",
        "- If a feature is missing, it is treated as **absent (0)** by default.\n",
        "\n",
        "### **2. Explicit Handling Methods**\n",
        "To improve model performance, missing values can be handled using the following techniques:\n",
        "\n",
        "#### **a. Imputation with Mode (Most Frequent Value)**\n",
        "- Replace missing values with the most common value (0 or 1) in that feature.\n",
        "- Useful when the dataset has structured binary data.\n",
        "\n",
        "#### **b. Assigning a Neutral Probability**\n",
        "- Instead of treating missing values as 0, assign a neutral probability (e.g., 0.5) to balance the impact.\n",
        "\n",
        "#### **c. Use Indicator Variables**\n",
        "- Introduce a separate feature that indicates whether the value was missing.\n",
        "- Helps the model recognize patterns in missing data.\n",
        "\n",
        "#### **d. Ignore Missing Features During Probability Calculation**\n",
        "- Only consider available features while computing the likelihood for classification.\n",
        "\n",
        "### **Conclusion**\n",
        "- Bernoulli Naive Bayes treats missing values as **absent (0) by default**.\n",
        "- Preprocessing methods like **imputation, indicator variables, or ignoring missing features** can help improve classification performance.\n"
      ],
      "metadata": {
        "id": "02Ku3IHAMTU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q4. Can Gaussian Naive Bayes Be Used for Multi-Class Classification?**\n",
        "\n",
        "### **Yes, Gaussian Naive Bayes (GNB) can be used for multi-class classification.**\n",
        "\n",
        "### **How It Works:**\n",
        "- Gaussian Naive Bayes assumes that **each feature follows a normal (Gaussian) distribution** within each class.\n",
        "- It calculates the **likelihood** of a data point belonging to each class based on the Gaussian probability density function (PDF).\n",
        "- The class with the **highest posterior probability** is selected.\n",
        "\n",
        "### **Multi-Class Classification with GNB:**\n",
        "- GNB applies the **Naive Bayes classification rule** independently for each class.\n",
        "- It computes **P(Class | Features)** for each possible class and assigns the most probable class to the given instance.\n",
        "\n",
        "### **Example Use Cases:**\n",
        "- **Handwritten digit recognition (e.g., MNIST dataset)**\n",
        "- **Iris dataset classification**\n",
        "- **Sentiment analysis (when features are continuous variables like word embedding scores)**\n",
        "\n",
        "### **Conclusion:**\n",
        "- **Gaussian Naive Bayes is naturally suited for multi-class classification** because it treats each class separately and selects the most probable one.\n",
        "- It is **fast, simple, and effective** for problems where features are normally distributed.\n"
      ],
      "metadata": {
        "id": "QKnVnia9N0Zb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q5. Assignment: Spambase Dataset Classification Using Naive Bayes**\n",
        "\n",
        "## **Data Preparation:**\n",
        "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase).\n",
        "This dataset contains **email messages**, where the goal is to **predict whether a message is spam or not** based on several input features.\n",
        "\n",
        "## **Implementation:**\n",
        "- Implement **Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes** classifiers using the **scikit-learn** library in Python.\n",
        "- Use **10-fold cross-validation** to evaluate the performance of each classifier on the dataset.\n",
        "- Use the **default hyperparameters** for each classifier.\n",
        "\n",
        "## **Results:**\n",
        "Report the following **performance metrics** for each classifier:\n",
        "- **Accuracy**\n",
        "- **Precision**\n",
        "- **Recall**\n",
        "- **F1 score**\n",
        "\n",
        "## **Discussion:**\n",
        "- Discuss the results obtained.\n",
        "- Which variant of **Naive Bayes** performed the best?\n",
        "- Why do you think that is the case?\n",
        "- Are there any **limitations of Naive Bayes** that you observed?\n",
        "\n",
        "## **Conclusion:**\n",
        "- Summarize the findings.\n",
        "- Provide **suggestions for future work**.\n"
      ],
      "metadata": {
        "id": "fNfyKjCbOEQ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dxr9VabDoCzM",
        "outputId": "d5c1fb3a-781f-4b64-9093-14c627f47a22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BernoulliNB:\n",
            "  Accuracy: 0.8857\n",
            "  Precision: 0.8851\n",
            "  Recall: 0.8158\n",
            "  F1 Score: 0.8490\n",
            "\n",
            "MultinomialNB:\n",
            "  Accuracy: 0.7903\n",
            "  Precision: 0.7398\n",
            "  Recall: 0.7215\n",
            "  F1 Score: 0.7305\n",
            "\n",
            "GaussianNB:\n",
            "  Accuracy: 0.8203\n",
            "  Precision: 0.6983\n",
            "  Recall: 0.9575\n",
            "  F1 Score: 0.8076\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load Spambase dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
        "column_names = [\n",
        "    'word_freq_make', 'word_freq_address', 'word_freq_all', 'word_freq_3d', 'word_freq_our',\n",
        "    'word_freq_over', 'word_freq_remove', 'word_freq_internet', 'word_freq_order', 'word_freq_mail',\n",
        "    'word_freq_receive', 'word_freq_will', 'word_freq_people', 'word_freq_report', 'word_freq_addresses',\n",
        "    'word_freq_free', 'word_freq_business', 'word_freq_email', 'word_freq_you', 'word_freq_credit',\n",
        "    'word_freq_your', 'word_freq_font', 'word_freq_000', 'word_freq_money', 'word_freq_hp',\n",
        "    'word_freq_hpl', 'word_freq_george', 'word_freq_650', 'word_freq_lab', 'word_freq_labs',\n",
        "    'word_freq_telnet', 'word_freq_857', 'word_freq_data', 'word_freq_415', 'word_freq_85',\n",
        "    'word_freq_technology', 'word_freq_1999', 'word_freq_parts', 'word_freq_pm', 'word_freq_direct',\n",
        "    'word_freq_cs', 'word_freq_meeting', 'word_freq_original', 'word_freq_project', 'word_freq_re',\n",
        "    'word_freq_edu', 'word_freq_table', 'word_freq_conference', 'char_freq_;', 'char_freq_(',\n",
        "    'char_freq_[', 'char_freq_!', 'char_freq_$', 'char_freq_#', 'capital_run_length_average',\n",
        "    'capital_run_length_longest', 'capital_run_length_total', 'label'\n",
        "]\n",
        "df = pd.read_csv(url, header=None, names=column_names)\n",
        "\n",
        "# Split features and labels\n",
        "X = df.drop(columns=['label'])\n",
        "y = df['label']\n",
        "\n",
        "# Initialize classifiers\n",
        "models = {\n",
        "    \"BernoulliNB\": BernoulliNB(),\n",
        "    \"MultinomialNB\": MultinomialNB(),\n",
        "    \"GaussianNB\": GaussianNB()\n",
        "}\n",
        "\n",
        "# Perform 10-fold cross-validation and compute metrics\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    y_pred = cross_val_predict(model, X, y, cv=skf)\n",
        "    accuracy = accuracy_score(y, y_pred)\n",
        "    precision = precision_score(y, y_pred)\n",
        "    recall = recall_score(y, y_pred)\n",
        "    f1 = f1_score(y, y_pred)\n",
        "    results[name] = {\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1 Score\": f1\n",
        "    }\n",
        "\n",
        "# Display results\n",
        "for name, metrics in results.items():\n",
        "    print(f\"{name}:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Discussion**\n",
        "\n",
        "### **Results Obtained:**\n",
        "From the results, we observe that **Bernoulli Naive Bayes (BNB) achieved the highest accuracy (0.8857)**, outperforming both **Multinomial Naive Bayes (MNB) and Gaussian Naive Bayes (GNB)** in terms of overall performance.\n",
        "\n",
        "### **Best Performing Variant:**\n",
        "- **Bernoulli Naive Bayes (BNB) performed the best** in terms of **accuracy (0.8857), precision (0.8851), recall (0.8158), and F1-score (0.8490)**.\n",
        "- **Gaussian Naive Bayes (GNB) had the highest recall (0.9575)** but lower precision (0.6983), indicating that it classified most spam emails correctly but also had a higher rate of false positives.\n",
        "- **Multinomial Naive Bayes (MNB) performed the worst** with **accuracy (0.7903)** and **F1-score (0.7305)**, suggesting it struggled with distinguishing spam from non-spam emails.\n",
        "\n",
        "### **Why Did BNB Perform the Best?**\n",
        "- **Bernoulli Naive Bayes is well-suited for binary feature representations**, which makes it effective for datasets where presence/absence of words is more significant than their frequency.\n",
        "- The **Spambase dataset contains binary-like features**, making BernoulliNB a good fit.\n",
        "- **Multinomial Naive Bayes works better when word frequency matters**, but in this case, it seems that the presence of certain words was more important than their counts.\n",
        "- **Gaussian Naive Bayes assumes continuous data**, but the dataset consists mostly of discrete features, which likely caused it to underperform in accuracy.\n",
        "\n",
        "### **Limitations Observed:**\n",
        "- **BNB still had a recall of 0.8158**, meaning it missed some spam emails.\n",
        "- **GNB had a very high recall (0.9575) but poor precision (0.6983)**, leading to **more false positives**, which is undesirable in spam classification.\n",
        "- **Naive Bayes assumes feature independence**, which is rarely true in real-world text data. More sophisticated models could capture dependencies between words.\n",
        "\n",
        "---\n",
        "\n",
        "# **Conclusion**\n",
        "\n",
        "### **Summary of Findings:**\n",
        "- **Bernoulli Naive Bayes was the best-performing model** with the highest accuracy (0.8857) and balanced precision-recall tradeoff.\n",
        "- **Gaussian Naive Bayes had the highest recall (0.9575)** but at the cost of poor precision.\n",
        "- **Multinomial Naive Bayes underperformed in this scenario**, likely because word presence mattered more than frequency.\n",
        "\n",
        "### **Suggestions for Future Work:**\n",
        "- **Try feature engineering techniques** such as **TF-IDF** instead of raw word counts to see if it improves MNB’s performance.\n",
        "- **Experiment with hybrid models**, such as combining **Naive Bayes with Logistic Regression or Decision Trees** to improve classification.\n",
        "- **Use deep learning-based models** like **LSTMs or Transformers** for spam detection to see if they outperform Naive Bayes.\n",
        "- **Optimize thresholding techniques** to adjust the precision-recall tradeoff, especially for reducing false positives in GNB.\n",
        "\n",
        "Overall, **Bernoulli Naive Bayes is the best choice for the Spambase dataset**, but future work could explore **more advanced models** to further enhance performance.\n"
      ],
      "metadata": {
        "id": "i42VcbT6QS2C"
      }
    }
  ]
}
