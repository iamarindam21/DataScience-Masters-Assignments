{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Q1. What is the KNN algorithm?**\n",
        "\n",
        "## **Definition**\n",
        "The **K-Nearest Neighbors (KNN)** algorithm is a **supervised learning** method used for **classification and regression**. It predicts the output based on the **K closest data points** in the dataset.\n",
        "\n",
        "---\n",
        "\n",
        "## **How It Works**\n",
        "1. **Choose K** (number of neighbors).\n",
        "2. **Find the K closest points** using a distance measure (e.g., **Euclidean distance**).\n",
        "3. **Make a prediction**:\n",
        "   - **Classification**: Assign the most common class among the K neighbors.\n",
        "   - **Regression**: Take the average of the K neighbors' values.\n",
        "\n",
        "---\n",
        "\n",
        "## **Example**\n",
        "If we want to classify a new fruit as **apple** or **orange**, KNN will look at the **K closest fruits** and assign the majority class.\n",
        "\n",
        "\n",
        "\n",
        "## **Conclusion**\n",
        "KNN is useful for **pattern recognition and recommendation systems**, but it can be **slow for big datasets** and requires careful selection of **K and distance measures**.\n"
      ],
      "metadata": {
        "id": "EjoOOj8C_Uuh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q2. How do you choose the value of K in KNN?**\n",
        "\n",
        "## **Importance of K**\n",
        "The value of **K** (number of neighbors) determines the balance between **bias and variance** in the KNN algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "## **Methods to Choose K**\n",
        "1. **Rule of Thumb**:  \n",
        "   - A common starting point is **K = √(N)**, where **N** is the number of samples.\n",
        "   - For small datasets, **K is usually between 3 and 10**.\n",
        "\n",
        "2. **Odd vs. Even K**:  \n",
        "   - Choose an **odd K** to avoid ties in classification problems with **two classes**.\n",
        "\n",
        "3. **Cross-Validation**:  \n",
        "   - Use **K-fold cross-validation** to find the best K.\n",
        "   - Try different values and choose the one with the **highest accuracy**.\n",
        "\n",
        "4. **Elbow Method**:  \n",
        "   - Plot the **error rate vs. K**.\n",
        "   - The best K is at the **\"elbow\" point** where the error stops decreasing significantly.\n",
        "\n",
        "## **Conclusion**\n",
        "The best **K** depends on the dataset. **Cross-validation and the elbow method** help in selecting an optimal value.\n"
      ],
      "metadata": {
        "id": "7qm8_IQ4SzpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q3. What is the difference between KNN Classifier and KNN Regressor?**\n",
        "\n",
        "K-Nearest Neighbors (**KNN**) can be used for both **classification** and **regression** tasks. The key difference lies in how predictions are made.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. KNN Classifier**\n",
        "- **Used for:** **Categorical** (discrete) target variables.\n",
        "- **Prediction Method:**  \n",
        "  - Finds the **K nearest neighbors** of a data point.\n",
        "  - Assigns the **most common class** among the neighbors.\n",
        "- **Example:**  \n",
        "  - Predicting if an email is **spam or not spam**.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. KNN Regressor**\n",
        "- **Used for:** **Continuous** target variables.\n",
        "- **Prediction Method:**  \n",
        "  - Finds the **K nearest neighbors**.\n",
        "  - Takes the **average (or weighted average)** of their values as the prediction.\n",
        "- **Example:**  \n",
        "  - Predicting the **price of a house** based on its size and location.\n",
        "\n",
        "\n",
        "## **Conclusion**\n",
        "- We Use **KNN Classifier** for **categorical predictions** (e.g., spam detection).\n",
        "- We Use **KNN Regressor** for **continuous predictions** (e.g., house price estimation).\n"
      ],
      "metadata": {
        "id": "rGa-pGJgTyle"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q4. How do you measure the performance of KNN?**\n",
        "\n",
        "The performance of the **K-Nearest Neighbors (KNN)** algorithm is evaluated differently for **classification** and **regression** tasks.  \n",
        "\n",
        "---\n",
        "\n",
        "###  **1. Performance Metrics for KNN Classifier**\n",
        "For classification problems, we use the following metrics:\n",
        "\n",
        "- **Accuracy:**  \n",
        "  - Measures the percentage of correctly classified instances.  \n",
        "  - Formula:  \n",
        "    \\[\n",
        "    \\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}}\n",
        "    \\]\n",
        "  \n",
        "- **Precision, Recall, and F1-Score:**  \n",
        "  - Used when class imbalance is present.\n",
        "  - **Precision**: Measures how many predicted positives are actually correct.  \n",
        "    \\[\n",
        "    \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
        "    \\]\n",
        "  - **Recall**: Measures how many actual positives were correctly predicted.  \n",
        "    \\[\n",
        "    \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
        "    \\]\n",
        "  - **F1-Score**: Harmonic mean of precision and recall.  \n",
        "    \\[\n",
        "    F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "    \\]\n",
        "\n",
        "- **Confusion Matrix:**  \n",
        "  - A table that shows the number of **true positives, false positives, true negatives, and false negatives**.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Performance Metrics for KNN Regressor**\n",
        "For regression problems, we use the following metrics:\n",
        "\n",
        "- **Mean Squared Error (MSE):**  \n",
        "  - Measures the average squared difference between predicted and actual values.\n",
        "  - Formula:  \n",
        "    \\[\n",
        "    MSE = \\frac{1}{n} \\sum (y_{\\text{true}} - y_{\\text{pred}})^2\n",
        "    \\]\n",
        "\n",
        "- **Root Mean Squared Error (RMSE):**  \n",
        "  - Square root of MSE, useful for interpreting error in the same unit as the target variable.\n",
        "  - Formula:  \n",
        "    \\[\n",
        "    RMSE = \\sqrt{MSE}\n",
        "    \\]\n",
        "\n",
        "- **Mean Absolute Error (MAE):**  \n",
        "  - Measures the average absolute difference between predicted and actual values.\n",
        "  - Formula:  \n",
        "    \\[\n",
        "    MAE = \\frac{1}{n} \\sum |y_{\\text{true}} - y_{\\text{pred}}|\n",
        "    \\]\n",
        "\n",
        "- **R² Score (Coefficient of Determination):**  \n",
        "  - Measures how well the model explains the variance in the target variable.\n",
        "  - Formula:  \n",
        "    \\[\n",
        "    R^2 = 1 - \\frac{\\sum (y_{\\text{true}} - y_{\\text{pred}})^2}{\\sum (y_{\\text{true}} - \\bar{y})^2}\n",
        "    \\]\n",
        "  - R² value closer to **1** means better performance.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "- We Use **accuracy, precision, recall, and F1-score** for classification problems.\n",
        "- We Use **MSE, RMSE, MAE, and R²** for regression problems.\n",
        "- Choosing the right metric depends on the problem, dataset characteristics, and business requirements.\n"
      ],
      "metadata": {
        "id": "M09692GLUPYW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q5. What is the curse of dimensionality in KNN?**\n",
        "\n",
        "The **curse of dimensionality** refers to the problems that arise when the number of features (dimensions) in a dataset increases. In the **K-Nearest Neighbors (KNN) algorithm**, this effect significantly impacts performance and accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### **How Does the Curse of Dimensionality Affect KNN?**\n",
        "\n",
        "### **a) Increased Distance Between Data Points**\n",
        "- In high-dimensional spaces, data points become more **sparse**.\n",
        "- The Euclidean distance between points becomes **less meaningful**, as all points tend to be nearly equidistant.\n",
        "- This makes it harder for KNN to find truly **\"nearest\"** neighbors.\n",
        "\n",
        "### **b) Increased Computational Cost**\n",
        "- KNN calculates the distance between a test point and all training points.\n",
        "- As the number of dimensions **increases**, the computational complexity **grows**, making it **slower**.\n",
        "\n",
        "### **c) Decreased Model Performance**\n",
        "- With too many dimensions, irrelevant or noisy features can **mislead the model**.\n",
        "- The model may become **less accurate** because the **nearest neighbors are not truly representative** of the target class.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "X_23KzMGU0qm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q6. How do you handle missing values in KNN?**\n",
        "\n",
        "Handling missing values in **K-Nearest Neighbors (KNN)** is crucial because KNN relies on distance-based calculations, and missing values can distort these distances.\n",
        "\n",
        "---\n",
        "\n",
        "### ** Methods to Handle Missing Values in KNN**\n",
        "\n",
        "### **a) Remove Instances with Missing Values**\n",
        "- If **only a few rows** have missing values, they can be **dropped**.\n",
        "- This is **not recommended** if many values are missing, as it reduces the dataset size.\n",
        "\n",
        "### **b) Imputation Using Mean, Median, or Mode**\n",
        "- Replace missing values with the **mean** (for continuous data), **median** (if skewed), or **mode** (for categorical data).\n",
        "\n"
      ],
      "metadata": {
        "id": "o__QDA-VVXzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?**\n",
        "\n",
        "## **1. KNN Classifier vs. KNN Regressor**\n",
        "K-Nearest Neighbors (KNN) can be used for both **classification** and **regression** tasks. The choice between the two depends on the type of problem.\n",
        "\n",
        "| Feature              | KNN Classifier | KNN Regressor |\n",
        "|----------------------|---------------|--------------|\n",
        "| **Type of Problem**  | Used for **categorical** target variables (e.g., spam detection, image classification). | Used for **continuous** target variables (e.g., predicting house prices, temperature). |\n",
        "| **Prediction Output** | Assigns the majority class among k-nearest neighbors. | Takes the **mean** (or weighted mean) of k-nearest neighbors. |\n",
        "| **Distance Calculation** | Uses distance metrics (e.g., Euclidean) to find nearest neighbors and assigns the most frequent class. | Uses distance metrics to find nearest neighbors and averages their values. |\n",
        "| **Handling Outliers** | More robust to outliers since classification depends on majority voting. | Sensitive to outliers since they can significantly affect the mean of neighbors. |\n",
        "| **Performance on Small Data** | Performs well, especially when data is not linearly separable. | Performs well on small datasets but struggles with high variance if k is too small. |\n",
        "| **Computational Cost** | Slower for large datasets (high-dimensional spaces). | Also computationally expensive for large datasets. |\n",
        "\n",
        "---\n",
        "## **Which One is Better for Which Type of Problem?**\n",
        "\n",
        "### **1. KNN Classifier** is better for:\n",
        "- **Categorical problems** where the target variable has discrete labels.\n",
        "- **Examples:**\n",
        "  - Spam detection (Spam or Not Spam).\n",
        "  - Disease diagnosis (Positive or Negative).\n",
        "  - Image classification (Dog, Cat, Bird, etc.).\n",
        "  - Customer segmentation (High-value, Medium-value, Low-value).\n",
        "\n",
        "### **2. KNN Regressor** is better for:\n",
        "- **Continuous problems** where the target variable is a real number.\n",
        "- **Examples:**\n",
        "  - Predicting house prices based on features like square footage and location.\n",
        "  - Estimating stock prices or sales revenue.\n",
        "  - Forecasting temperature or weather conditions.\n",
        "  - Predicting customer lifetime value.\n",
        "\n",
        "### **Conclusion:**\n",
        "- We need to Use **KNN Classifier** for **classification** problems where outputs belong to distinct categories.\n",
        "- We need to  **KNN Regressor** for **regression** problems where outputs are continuous numerical values.\n"
      ],
      "metadata": {
        "id": "WJWq5i5yWIoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?**\n",
        "\n",
        "### **Strengths:**\n",
        "1. **Simple and Intuitive:**\n",
        "   - Easy to understand and implement.\n",
        "2. **Non-Parametric:**\n",
        "   - No assumption about data distribution, making it flexible for various datasets.\n",
        "3. **Works Well with Small Datasets:**\n",
        "   - Performs well when the number of samples is limited.\n",
        "4. **Adaptable to Different Problems:**\n",
        "   - Can be used for both classification and regression.\n",
        "5. **Handles Multi-Class Problems:**\n",
        "   - Efficient for multi-class classification without any modifications.\n",
        "\n",
        "\n",
        "\n",
        "### **Weaknesses & How to Address Them:**\n",
        "\n",
        "1. **Computational Cost:**  \n",
        "   - KNN requires calculating distances for every query, making it slow for large datasets.  \n",
        "   - **Solution:** Use efficient data structures like KD-Trees or Ball Trees to speed up nearest neighbor searches.\n",
        "\n",
        "2. **Memory Intensive:**  \n",
        "   - KNN stores the entire training dataset, leading to high memory usage.  \n",
        "   - **Solution:** Apply dimensionality reduction techniques like PCA to reduce the number of features and optimize storage.\n",
        "\n",
        "3. **Curse of Dimensionality:**  \n",
        "   - As the number of features increases, distance calculations become less meaningful, reducing accuracy.  \n",
        "   - **Solution:** Use feature selection or dimensionality reduction to retain only the most relevant features.\n",
        "\n",
        "4. **Sensitive to Noisy Data:**  \n",
        "   - Outliers and irrelevant features can significantly impact predictions.  \n",
        "   - **Solution:** Perform data preprocessing, such as removing outliers, normalizing data, and using weighted KNN to reduce the impact of noisy points.\n",
        "\n",
        "5. **Imbalanced Data Issues:**  \n",
        "   - KNN is biased toward the majority class in imbalanced datasets.  \n",
        "   - **Solution:** Use weighted KNN to give more importance to minority class points or apply resampling techniques like SMOTE.\n",
        "\n",
        "6. **Choice of K Affects Performance:**  \n",
        "   - A small K leads to overfitting, while a large K results in underfitting.  \n",
        "   - **Solution:** Use cross-validation to find the optimal K value that balances bias and variance.\n",
        "\n",
        "\n",
        "### **Conclusion:**\n",
        "- KNN is a powerful algorithm for classification and regression, but it suffers from computational inefficiency and sensitivity to noise.\n",
        "- Optimizing K, using efficient data structures, and applying preprocessing techniques can help overcome these limitations.\n"
      ],
      "metadata": {
        "id": "HxcJBQ92XvGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?**\n",
        "\n",
        "### **Euclidean Distance:**\n",
        "- Measures the straight-line (shortest) distance between two points in a multi-dimensional space.\n",
        "- Formula:  \n",
        "  \\[\n",
        "  d(p, q) = \\sqrt{\\sum_{i=1}^{n} (q_i - p_i)^2}\n",
        "  \\]\n",
        "- Best suited for continuous and dense data.\n",
        "- More sensitive to outliers due to squaring differences.\n",
        "\n",
        "### **Manhattan Distance:**\n",
        "- Measures the distance between two points along grid-like paths (sum of absolute differences).\n",
        "- Formula:  \n",
        "  \\[\n",
        "  d(p, q) = \\sum_{i=1}^{n} |q_i - p_i|\n",
        "  \\]\n",
        "- Works well for high-dimensional and sparse data.\n",
        "- Less sensitive to outliers compared to Euclidean distance.\n",
        "\n",
        "### **When to Use Which?**\n",
        "- **We Use Euclidean Distance** when the data points are continuous and evenly distributed.\n",
        "- **We Use Manhattan Distance** when the data has grid-like properties (e.g., city blocks) or is high-dimensional and sparse.\n"
      ],
      "metadata": {
        "id": "yNWXZchLZhYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q10. What is the role of feature scaling in KNN?**\n",
        "\n",
        "### **Role of Feature Scaling in KNN:**\n",
        "KNN is a distance-based algorithm that calculates the similarity between data points using metrics like **Euclidean distance** or **Manhattan distance**. If the features in the dataset have different scales, features with larger numerical ranges will dominate the distance calculation, leading to biased predictions.\n",
        "\n",
        "### **Why is Feature Scaling Important?**\n",
        "1. **Prevents Dominance of Large-Scale Features:**  \n",
        "   - Features with larger values (e.g., salary in thousands vs. age in years) can disproportionately influence distance calculations.\n",
        "   \n",
        "2. **Ensures Fair Contribution of All Features:**  \n",
        "   - Scaling normalizes all features to a comparable range, preventing bias toward a particular feature.\n",
        "\n",
        "3. **Improves Accuracy and Performance:**  \n",
        "   - Leads to better neighbor selection and enhances model performance.\n",
        "\n",
        "4. **Speeds Up Computation:**  \n",
        "   - Reduces computation time by keeping distances within a manageable range.\n",
        "\n",
        "### **Common Feature Scaling Methods:**\n",
        "- **Min-Max Scaling:**  \n",
        "  \\[\n",
        "  X' = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\n",
        "  \\]\n",
        "  - Scales features to a range [0,1] or [-1,1].\n",
        "  - Useful when feature distributions are not normal.\n",
        "\n",
        "- **Standardization (Z-score normalization):**  \n",
        "  \\[\n",
        "  X' = \\frac{X - \\mu}{\\sigma}\n",
        "  \\]\n",
        "  - Centers the data around 0 with a standard deviation of 1.\n",
        "  - Works well for normally distributed data.\n",
        "\n",
        "### **Conclusion:**\n",
        "Feature scaling is essential in KNN to ensure fair distance computation and improve model accuracy. Standardization or normalization should always be applied before using KNN.\n"
      ],
      "metadata": {
        "id": "0i431njPZ6hE"
      }
    }
  ]
}